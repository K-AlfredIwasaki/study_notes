---
title: "Week 11 Live Session"
author: "w203 Instructional Team"

date: "November 16, 2016"
output: pdf_document
---

## Announcements



*********************************

## Multiple linear regression (MLR) 

A simple linear regression uses a single predictor variable (X) to model the response variable (Y). In the real world, relationships are often more complex and more than one factor can influence the outcome of an event (i.e. the response variable). A multiple linear regression describes how a single response variable Y depends linearly on a number of predictor variables.

For example, a single linear regression can be used to show the impact of location on housing prices. A multiple linear regression can also factor in other factors that may have a stronger impact on the price, such as the number of bathrooms, number bedrooms, the number of bathrooms, the year the house was built, the square footage, etc.

We will use the term multiple regression for models with two or more predictors and one response (There are also regression models with two or more response variables, which are called multivariate regression models).

###Interpreting coefficients in multiple regression correctly

Coefficients in a multiple regression have a ceteris paribus interpretation - holding all other independent variables (and the error) constant.

In the example $$\hat{y} = 1 + 2x_1 + 3x_2$$ It is not accurate to say "For each change of 1 unit in $x_1$, $y$ changes 2 units". What is correct is to say, "If $x_2$ is fixed, then for each change of 1 unit in $x_1$, the conditional mean of $y|x$ changes 2 units."

Q1.1: Give an example of variables $y, x_1,$ and $x_2$, such that the coefficient on $x_1$ is likely to be positive in a simple regression, but drops to zero or negative when $x_2$ is added.


<!-- After fitting a regression model it is important to determine whether all the necessary model assumptions are valid before performing inference. If there are any violations, subsequent inferential procedures may be invalid resulting in faulty conclusions. Hence, performing the appropriate model diagnostics is crucial for validating the results. -->


<!-- Q1.2: What are the key assumptions used for multiple regression models? 
Paul: I took this one out since there's a more complete discussion below.-->
<!-- A key assumption used for regression models is that the response variable is to the explanatory variables were linear in the parameters and that the errors were independent and identically distributed (i.i.d) normal random variables with mean 0 and constant variance. -->

#-----------

## OLS Estimators

Remember that a regression coefficient based on sample data is an estimate of a true regression parameter for the population the sample is drawn from. There are several desireable properties that we might want our coefficients to have.  These properties require different sets of assumptions to hold.

Q2.1: What does it mean for an estimator to be unbiased?
<!-- An estimator is unbiased if its expected value (mean) is the same as the true parameter value in the population. In other words, an unbiased estimator  -->

Q2.2: What does it mean for an estimator to be consistent?
<!-- An estimator of a parameter is consistent if the estimate converges to the true value of the parameter as the sample size increases. I.e. its accuracy tends to improve as the sample size grows larger.  -->

Q2.3: What does it mean for an estimator to be efficient?
<!-- Efficiency refers to the accuracy of the estimates produced by the estimator. An estimator may be referred to as efficient if it is the most accurate (i.e., -->
<!-- its variance is the smallest) of all unbiased estimators for the given parameter.  -->

Q2.4: What other properties might we want our OLS coefficients to have?
<!-- We will want to know the sampling distribution of our statistics so we can test hypotheses, construct confidence intervals, and so forth. -->


#-----------

<!-- Aside from these three properties, it is also often desirable to assume a particular probability distribution for the sampling distribution of a given test statistic. Recall that a sampling distribution is the distribution of a particular statistic over repeated samplings from a population. The validity of this assumption, however, depends on the assumption of normally distributed model errors (at least when working with small samples).
Paul: I took this out because it refers to week 12 material.
-->

To show that the coefficients in a multiple regression are unbiased, we need four assumptions, which are extension of the assumptions we learned for simple regression.

Q3.1:  What are these four assumptions?
<!-- 1. Linear in Parameters -->
<!-- 2. Random Sampling -->
<!-- 3. No Perfect Collinearity -->
<!-- 4. Zero Conditional Mean -->

Q3.2: What are the implications of these four assumptions, both conceptually and mathematically?

#Associative versus Causal models

Q4.1 What is the difference between the following two conditions?

1. $cov(x, \hat{u}) = 0$

2. $cov(x, u) = 0$

### Associative models

An associative model means that we don't care about causality.  We just want to estimate the line of best fit in the population.  This is the line that best fits the joint distribution of X and Y.

This makes life easy, because if we define our errors as the distance from this line, we would find that they fulfill $cov(x, u) = 0$.  This condition is exactly the same as the exogeneity condition (CLM 4\*).  This means that as long as we also fulfill CLM 1-3, OLS will estimate this line *consistently*.

In Figure 1, we have a heatmap showing the joint distribution of umbrella sales and rainfall.  This is a toy example that's problematic in many ways (do we have measurements across time? across locations?) but it helps to clarify what's happening.

![Predicting Rainfall from Umbrella Sales](umbrellas.png)

Q4.2 How would you draw the line of best fit in the population on the Umbrella heatmap?  Which of the two conditions from Q4.1 does this line fulfill?

Q4.3 Suppose you collect a set of observations from this population model and fit an ols regression line to it.  How does this line compare to the line above?  Which of the two conditions from Q4.1 does this line fulfill?

###Causal models

Causal modeling is a huge topic and we will explain just a little about how we think about it intuitively.  Note that the Wooldridge text generally takes a causal approach to modeling. 

In a causal/structural approach, we believe that if we could just measure all the factors that are out there and put them into a regression equation (in the right way), our parameters will have a causal interpretation.  In the umbrella example, the problem would be that there are atmospheric factors that seem to be missing from the model.  Suppose the real population model is the following,

$$rainfall = 0 \cdot umbrellas + 20 \cdot humidity - 10 \cdot airpressure + u$$

where u is uncorrelated with all other predictor variables.  This model tells us that buying an extra umbrella has zero effect on rainfall.

Q4.4 How would you draw a line representing the causal effect of umbrellas on rainfall on the heatmap below?

Q4.5 Assuming that we haven't measured humidity or airpressure, can OLS regression estimate this line?

Since we didn't measure humidity or airpressure, they become part of the error in our simple regression.  A causal modeler would then say that umbrellas is correlated with the error - this is a condition known as *endogeneity*.


##Model Diagnostics
Model diagnostic procedures, both graphical methods and formal statistical tests, are important for checking our model. These procedures allow us to explore whether the assumptions of the regression model
are valid and decide whether we can trust subsequent inference results.


What is the value in examining a scatter plot for a regression analysis?

<!-- Residual scatter plots provide a visual examination of the assumption homoscedasticity between the predicted dependent variable scores and the errors of prediction.  The primary benefit is that the assumption can be viewed and analyzed with one glance; therefore, any violation can be determined quickly and easily.  When an analysis meets the assumptions, the chances for making Type I and Type II errors are reduced, which improves the accuracy of the research findings. -->

<!-- A residual scatter plot is a figure that shows one axis for predicted scores and one axis for errors of prediction.  Initial visual examination can isolate any outliers, otherwise known as extreme scores, in the data-set.  Tabachnick and Fidell (2007) explain the residuals (the difference between the obtained DV and the predicted DV scores) and the variance of the residuals should be the same for all predicted scores (homoscedasticity).  If this is true, the assumption is met and the scatter plot takes the (approximate) shape of a rectangular; scores will be concentrated in the center (about the 0 point) and distributed in a rectangular pattern.  More simply, scores will be randomly scattered about a horizontal line. In contrast, any systematic pattern or clustering of scores is considered a violation. -->


### Residuals Plots

A residuals plot can be used to assess the assumption that the variables have a linear relationship. The plot is formed by graphing the standardized residuals on the y-axis and the standardized predicted values on the x-axis. An optional horizontal line can be added to aid in interpreting the output.

###Influential Observations

Q4.1 What does it mean for a data point to have high leverage?

Q4.2 What does it mean for a data point to have high influence?

Q4.3 How is it possible for a data point to have low leverage but high influence?

Q4.4 How is it possible for an outlier to have low influence?






#R Exercise

The file crime1.RData contains individual-level data on criminal activity and punishment.  It was used in the paper, J. Grogger (1991), “Certainty vs. Severity of Punishment,” Economic Inquiry 29, 297-309.

You are interested in predicting the number of times an individual was arrested in 1986 (narr86).  Your dependent variables will be the proportion of prior arrests that led to conviction (pcnv) and the average previous sentence length in months (avgsen).

```{r}
load("crime1.RData")
desc
```

1. First, you are interested in the simple regression of narr86 on pcnv.  Examine these variables and run a linear regression.


2. Generate a residuals vs. fitted values plot.  Assess the assumptions for unbiasedness of OLS.

3. Generate a residuals vs. leverage plot.  Assess whether any observations are exerting unusual influence on the regression coefficient.

4. Interpret your regression coefficients.

---------

5. Next, you are interested in adding variable avgsen to your model.  However, instead of fitting a linear model directly in R, use the regression anatomy formula to predict what the slope coefficient for avgsen would be in the multiple regression.

Recall that the coefficient on an independent variable $x_i$ is equal to,

$$\beta_i = cov(y, r_i) / var(r_i)$$

where $r_i$ is the residual from regressing $x_i$ on all other independent variables.

- First, regress avgsen on pcnv and extract the residuals from this regression.

- Next, regress narr86 on the residuals from your first stage.  What slope coefficient do you get?


6. Now, confirm your previous result by directly fitting the multiple regression model.  That is, run the regression of narr86 on both pcnv and avgsen.  What is the coefficient on avgsen?


7. Comment on the practical significance of your results.

8. How do you explain the sign of the coefficient on avgsen?  Explain whether this variable is exogenous.  Can your coefficient have a causal interpretation?

9. Use R to predict what number of arrests an individual would have, if half of their prior arrests led to convictions and their average sentence was 3 months.

10. Which of your regression models fits the data better?  Provide statistics to back up your response.

11. Place the results from both models in a regression table.

###Cheat Sheet for checking bias assumptions 

Linear population model: Nothing to check, because we haven't constrained the errors yet.

Random Sampling: Not interpretable from graphs.  You need information about where the data comes from.

No perfect multicollinearity:  This is a rare case that will trigger an error in R.

Zero-conditional mean: Check the Residual vs Fitted plot for a systematic depature from the horizontal line.


