knitr::opts_chunk$set(echo = TRUE)
#closeAllConnections()
#rm(list=ls())
library(car) # scatter plot
library(ggplot2) # plot
library(dplyr) # data manipulation
library(rpart) # regression tree
library(randomForest) # random forest
setwd("C:/Users/K/Desktop/Berkeley/01_Job hunting/02 Acing interviews/Takehome")
data <- read.csv("data/conversion_data.csv")
head(data)
str(data)
summary(data)
# check statstics and missing value
# check exteme values for the statistics
# there are two users over 100 years old
data[data$age > 100, ]
# remove the two outliers from the dataset
data <- data %>%
filter(age < 100)
# confirm the two outliers are removed
nrow(data)
# histogram
col_names <- names(data)
col_names <- col_names[! col_names %in% c("country", "source")]
for (i in col_names) {
p1 <- ggplot(data, aes_string(x = i)) +
geom_histogram(bins=100)
plot(p1)
}
col_names <- names(data)
col_names <- col_names[! col_names %in% c("converted")]
for (i in col_names) {
plt1 <-  ggplot(data, aes_string(x = i, y = "converted")) +
geom_jitter(alpha=0.5)
plot(plt1)
}
data_country = data %>%
group_by(country) %>%
summarise(conversion_rate = mean(converted))
ggplot(data = data_country, aes(x=country, y=conversion_rate)) +
geom_bar(stat="identity")
data_user = data %>%
group_by(new_user) %>%
summarise(conversion_rate = mean(converted))
ggplot(data = data_user, aes(x = new_user, y=conversion_rate)) +
geom_bar(stat="identity")
data_user = data %>%
group_by(source) %>%
summarise(conversion_rate = mean(converted))
ggplot(data = data_user, aes(x = source, y=conversion_rate)) +
geom_bar(stat="identity")
data_pages = data %>%
group_by(total_pages_visited) %>%
summarise(conversion_rate = mean(converted))
ggplot(data = data_pages, aes(x = total_pages_visited, y=conversion_rate)) +
geom_bar(stat="identity")
data_age = data %>%
group_by(age) %>%
summarise(conversion_rate = mean(converted))
ggplot(data = data_age, aes(x = age, y=conversion_rate)) +
geom_bar(stat="identity")
ggplot(data, aes(x = age, fill = as.factor(converted))) +
geom_histogram(bins = 50)
levels(data$data_country)
data$data_country
levels(data$country)
levels(data$country)[levels(data$country) == "Germany"] = "DE"
data$converted = as.factor(data$converted)
data$new_user = as.factor(data$new_user)
levels(data$country)[levels(data$country) == "Germany"] = "DE"
train_simple = sample(nrow(data), size = nrow(data) * 0.66)
train_data = data[train_sample,]
train_simple
train_simple = sample(nrow(data), size = nrow(data) * 0.66)
train_data = data[train_sample,]
train_sample = sample(nrow(data), size = nrow(data) * 0.66)
train_data = data[train_sample,]
test_data = data[-train_sample,]
rf = randomForest(y=train_data$converted, x = train_data[, -ncol(train_data)],
ytest = test_data$converted, xtest = test_data[, -ncol(test_data)],
ntree = 100, mtry = 3, keep.forest = TRUE)
rf
test_data$converted
naive_predict <- rep(0, length(test_data$converted))
naive_predict
library(caret)
install.packages("caret")
library(caret)
confusionMatrix(test_data$converted, naive_predict, positive = "1")
confusionMatrix(test_data$converted, as.factor(naive_predict), positive = "1")
naive_predict <- as.factor(rep(0, length(test_data$converted)))
confusionMatrix(test_data$converted, naive_predict, positive = "1")
str(naive_predict)
str(test_data$converted)
confusionMatrix(test_data$converted, naive_predict, positive = "1")
confusionMatrix(naive_predict, test_data$converted, positive = "1")
?confusionMatrix
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.voes[,2]
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.votes[,2]
pred.obj <- prediction(OOB.pred, y)
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.votes[,2]
pred.obj <- prediction (OOB.pred,y)
pred.obj <- prediction (OOB.pred, train_data$converted)
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.votes[,2]
pred.obj <- prediction (OOB.pred, train_data$converted)
install.packages("ROCR")
library(ROCR)
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.votes[,2]
library(ROCR)
pred.obj <- prediction (OOB.pred, train_data$converted)
PR.perf <- performance(pred.obj, "rec", "prec")
plot (RP.perf)
OOB.votes <- predict (rf,train_data[, -ncol(train_data)], type = "prob")
OOB.pred <- OOB.votes[,2]
library(ROCR)
pred.obj <- prediction (OOB.pred, train_data$converted)
RP.perf <- performance(pred.obj, "rec", "prec")
plot (RP.perf)
ROC.perf <- performance(pred.obj, "fpr", "tpr")
plot  (RP.perf@alpha.values[[1]],RP.perf@x.values[[1]])
lines (RP.perf@alpha.values[[1]],RP.perf@y.values[[1]])
lines (ROC.perf@alpha.values[[1]],ROC.perf@x.values[[1]])
varImpPlot(rf, type=2)
rf
rf = randomForest(y = train_data$converted, x = train_data[, -c(5, ncol(train_data))],
ytest = test_data$converted, xtest = test_data[, -c(5, ncol(train_data))],
ntree = 100, mtry = 3, keep.forest = TRUE, classwt = c(0.7, 0.3))
rf
varImpPlot(rf, type = 2)
op <- par(mfrwo = c(2, 2))
partialPlot(rf, train_data, country, 1)
partialPlot(rf, train_data, age, 1)
partialPlot(rf, train_data, new_user, 1)
partialPlot(rf, train_data, source, 1)
op <- par(mfrow = c(2, 2))
partialPlot(rf, train_data, country, 1)
partialPlot(rf, train_data, age, 1)
partialPlot(rf, train_data, new_user, 1)
partialPlot(rf, train_data, source, 1)
tree = rpart(data$converted ~., data[, -c(5, ncol(data))],
control = rpart.control(maxdepth = 3),
parms = list(prior = c(0.7, 0.3)))
tree
setwd("C:/Users/K/Desktop/Berkeley/00_Academics/01_2017 Summer/W203 STATS/week10")
setwd("C:/Users/K/Desktop/Berkeley/00_Academics/W203 STATS/week10")
set.seed(898)
x = rnorm(100, 10, 5)
u = rnorm(100, 0, 1)
y = 1 + 0.5 * x + u
plot(x, y, main = "Simulated Data from Linear Population Model")
(simmodel1 = lm(y ~ x))
summary(simmodel1)$r.square
summary(simmodel1)
head(cbind(u, u_hat))
u_hat = simmodel1$residuals
head(cbind(u, u_hat))
plot(x, u_hat, main = "Residual versus Predictor Simulated Data"))
plot(x, u_hat, main = "Residual versus Predictor Simulated Data"))
plot(x, u_hat, main = "Residual versus Predictor Simulated Data")
load("GPA1.rdata")
ls()
desc
head(data)
summary(data$ACT)
hist(data$ACT, breaks = 16:36 - 0.5,
main = "HIstogram of ACT scores", xlab = NULL)
hist(data$ACT, breaks = 16:36 - 0.5,
main = "HIstogram of ACT scores", xlab = NULL)
summary(data$GPA)
summary(data$GPA)
summary(data$colGPA)
hist(data$colGPA, breaks = 20, main = "Histogram of College GPA",
xlab = NULL)
plot(jitter(data$ACT), jitter(data$colGPA), xlab = "ACT score",
ylab = "college GPA", main = "College GPA verus ACT score")
(model1 = lm(col$GPA ~ ACT, data = data))
(model1 = lm(colGPA ~ ACT, data = data))
abline(model1)
summary(model1)$r.square
library(BSDA)
attach(Gpa)
summary(Gpa)
hist(HSGPA)
hist(CollGPA)
Y <- CollGPA
x <- HSGPA
library(BSDA)
install.packages("BSDA")
library(BSDA)
attach(Gpa)
summary(Gpa)
hist(HSGPA)
hist(CollGPA)
Y <- CollGPA
x <- HSGPA
model = lm(y~ x)
model = lm(Y~ x)
model
plot(x, y)
plot(x, Y)
corr(x, Y)
cor(x, Y)
sum(x- mean(x))
2**2
2**3
sum((x- mean(x))(Y-mean(Y))/ sum((x-mean(x))**2)
2**3
sum((x- mean(x))(Y-mean(Y))/ sum((x-mean(x))**2))
sum((x- mean(x))(Y-mean(Y))/ sum((x-mean(x))**2)
2**3
sum((x- mean(x))*(Y-mean(Y))/ sum((x-mean(x))**2)
2**3
sum((x- mean(x))*(Y-mean(Y))/ sum((x-mean(x))^2)
2**3
sum((x- mean(x))*(Y-mean(Y)))/ sum((x-mean(x))^2)
model = lm(Y~ x)
model
mean(Y) - b1 * mean(x)
b1 <- sum((x- mean(x))*(Y-mean(Y)))/ sum((x-mean(x))^2)
mean(Y) - b1 * mean(x)
plot(x, Y)
abline(model)
abline(model,col="blue",lwd=2)
u_hat = model$residuals
u_hat
cor(u_hat, Y)
cor(u_hat, x)
model$$r.square
model$r.square
model$r.square
model = lm(Y~ x)
model$r.square
model$r.squared
summary(model)$r.square
(cor(x, Y)) ^2
summary(model)
model$fitted.values
var(model$fitted.values)/var(Y)
tmp1 <- rnorm(10, 0, 10)
model2 <- lm(Y~x+tmp1)
summary(model2)r.square
model2 <- lm(Y~x+tmp1)
summary(model2)$r.square
x_out <- c(Gpa$HSGPA,3.4)
y_out <- c(Gpa$CollGPA, 1.5)
cor(x_out,y_out)
model_out <- lm(y_out ~ x_out)
plot(x_nl,y_nl)
x_nl<-seq(-25,25,1)
y_nl<-x_nl^2+rnorm(51,0,100)
plot(x_nl,y_nl)
