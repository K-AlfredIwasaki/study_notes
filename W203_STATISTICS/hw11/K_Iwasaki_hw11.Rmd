---
title: "hw11"
author: "K Iwasaki"
date: "July 23, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#closeAllConnections()
#rm(list=ls())

setwd("C:/Users/K/Desktop/Berkeley/00_Academics/W203 STATS/hw11")
load("Week11.rdata")
ls()

```
## Get familiar with the data

```{r}
Definitions
```


```{r}

head(Data)

```

```{r}
hist(Data$AG.LND.FRST.ZS)

hist(Data$AG.LND.FRST.ZS)
```

```{r}
hist(Data$NE.IMP.GNFS.CD) # Imports of goods and services (current US$)
hist(Data$NE.EXP.GNFS.CD) # Exports of goods and services (current US$)
```


```{r}
hist(Data$AG.LND.FRST.ZS) # Forest area (% of land area)

```

Notice there are many NA values in some columns

```{r}
summary(Data)
```
### Run: apply(!is.na(Data[,-(1:2)] ) , MARGIN= 2, mean ) and explain what it is showing.

The line of code computes the percentage of non-NA values in each column. For example, AG.LND.FRST.ZS has 264 - 8 non-NAs out of 264 rows = 0.9696

```{r}
apply(!is.na(Data[,-(1:2)] ) , MARGIN= 2, mean )

nrow(Data) # number of rows 264

```

### Can you include both NE.IMP.GNFS.CD and NE.EXP.GNFS.CD in the same OLS model? Why?
No. The plot below shows that these variable have strong linear relationship. This breaks no multicollearity assumption for OLS estimators if they are included in the same model.

```{r}
plot(Data$NE.EXP.GNFS.CD, Data$NE.IMP.GNFS.CD)
```


### Rename the variable named AG.LND.FRST.ZS to forest. This is going to be our dependent variable.

```{r}
colnames(Data)[colnames(Data) == "AG.LND.FRST.ZS"] = "forest"
#colnames(Data)

```

# Decribe a model for that predicts forest

### Write a model with two explanatory variables
Create a residuals versus fitted values plot and assess whether your coefficients are unbiased

It turns out that the residual plot shows that the mean residual doesn't change with the fitted values. The spread of the residulas are constant. This confirms that two OLS assumptions: 1) error term has a zero conditional mean and 2) Error term has a constant variance (homoskedasticity).

In addition, assume that linearity in model parameters, random sampling and variability, and no perfect collinearity among variables. This OLS estimator is unbiased.

```{r}
model = lm(forest ~ log(NE.EXP.GNFS.CD) + log(NY.GDP.PCAP.CD), Data)
plot(model, which =1)

```

How many observations are being used in your analysis?

```{r}
length(model$residuals)
```


Are the countries that are dropping out by random chance? If not, what would this do to our inference?

There are not many duplications for the countries that are dropping out for two variables I selected in the model. Thus, I assume the countries that are dropping out by random chance.

If this is violated, we cannot maintain that the OLS estimator is unbiased.

```{r}
Data$Country.Name[is.na(Data$NE.EXP.GNFS.CD)]

Data$Country.Name[is.na(Data$NY.GDP.PCAP.CD)]

```

### Now add a third variable

```{r}
model2 = lm(forest ~ log(NE.EXP.GNFS.CD) + log(NY.GDP.PCAP.CD) + NY.GDP.PCAP.CD, Data)
model2
plot(model2, which =1)
```

### Show how you would use the regression anatomy formula to compute the coefficient on your third variable. First, regress the third variable on your first two variables and extract the residuals. Next, regress forest on the residuals from the first stage.

```{r}

fs = lm(NY.GDP.PCAP.CD ~ log(NE.EXP.GNFS.CD) + log(NY.GDP.PCAP.CD), Data)
fs


y = Data$forest[!is.na(Data$NY.GDP.PCAP.CD) & !is.na(Data$NE.EXP.GNFS.CD) & !is.na(Data$NY.GDP.PCAP.CD)]

ra = lm(y ~ fs$residuals)
ra


```

### Compare your two models. Do you see an improvement? Explain how you can tell.

Use Akaike information criterion (AIC) for the assessment. Since AIC score decreases from model1 to model2 which is with additional variable. Model1 is better.

```{r}
AIC(model)
AIC(model2)

```

# Make up a country

### Make up a country named Mediland which has every indicator set at the median value observed in the data.
```{r}
Mediland = apply(Data[,-(1:3)] , MARGIN= 2, mean, na.rm = TRUE)
str(Mediland)

```


### How much forest would this country have?
```{r}

x = data.frame(NE.EXP.GNFS.CD = Mediland[5], NY.GDP.PCAP.CD = Mediland[7])
predict(model, x)

```


# Take away
### What is the causal story, if any, that you can take away from the above analysis? Explain why

In a causal/structural approach, we believe that if we could just measure all the factors that are out there and put them into a regression equation (in the right way), our parameters will have a causal interpretation.

In this example, the models tells that increase in GDP per capita causes forest to increase and increase in Exports of goods and services causes forest to descrease. 












